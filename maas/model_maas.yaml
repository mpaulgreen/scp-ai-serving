# Example LLM Model Deployment
# This demonstrates how to deploy a model using the MaaS platform

---
apiVersion: v1
kind: Namespace
metadata:
  name: llm

---
# LLMInferenceService with MaaS Gateway Integration
apiVersion: serving.kserve.io/v1alpha1
kind: LLMInferenceService
metadata:
  name: facebook-opt-125m-simulated
  namespace: llm
  annotations:
    alpha.maas.opendatahub.io/tiers: '[]'
spec:
  # Model configuration
  model:
    name: facebook/opt-125m
    uri: hf://facebook/opt-125m

  # Number of replicas
  replicas: 1

  template:
    containers:
    - name: main
      image: ghcr.io/llm-d/llm-d-inference-sim:v0.5.1
      imagePullPolicy: Always
      command:
      - /app/llm-d-inference-sim
      args:
      - --port
      - "8000"
      - --model
      - facebook/opt-125m
      - --mode
      - random
      - --ssl-certfile
      - /var/run/kserve/tls/tls.crt
      - --ssl-keyfile
      - /var/run/kserve/tls/tls.key
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      ports:
      - containerPort: 8000
        protocol: TCP
      livenessProbe:
        httpGet:
          path: /health
          port: 8000
          scheme: HTTPS
        initialDelaySeconds: 30
        periodSeconds: 10
        timeoutSeconds: 5
        failureThreshold: 3
      readinessProbe:
        httpGet:
          path: /health
          port: 8000
          scheme: HTTPS
        initialDelaySeconds: 5
        periodSeconds: 5
        timeoutSeconds: 3
        failureThreshold: 3
  router:
    gateway:
      refs:
      - name: maas-default-gateway
        namespace: openshift-ingress
    route: {}
